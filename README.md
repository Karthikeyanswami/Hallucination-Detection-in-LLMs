# Hallucination Detection in Large Language Models (LLMs)

This repository contains the mini project titled **"Detecting Hallucinations in Large Language Models"**, which explores a machine learning-based approach to identifying hallucinated (factually incorrect or fabricated) outputs generated by large-scale language models. The project emphasizes the need for factual reliability in AI-generated text and provides a pipeline to classify outputs as hallucinated or factual using traditional and ensemble-based classifiers.

## Problem Statement

Large Language Models (LLMs), while powerful in generating coherent and fluent responses, are known to occasionally produce hallucinationsâ€”statements that are syntactically correct but factually inaccurate or unsupported by real-world knowledge. These hallucinations can mislead users, especially in sensitive domains like healthcare, law, and academia. The aim of this project is to build a robust detection system that can accurately identify hallucinated outputs using a combination of textual features, encoding methods, and ensemble learning techniques.

## Repository Structure

- `mini_project_report.pdf`: Comprehensive project report detailing methodology, experiments, and results.
- `hallucination_detection.ipynb`: Jupyter notebook with complete code for preprocessing, model training, and evaluation.
- `README.md`: Overview and documentation of the project.

## Key Features

**Dataset**  
The dataset consists of AI-generated sentences labeled as hallucinated or non-hallucinated. Preprocessing steps include text normalization, vectorization (TF-IDF and BERT), and handling class imbalance through sampling techniques.

**Models Implemented**  
- Logistic Regression  
- Random Forest  
- K-Nearest Neighbors  
- AdaBoost  
- Gradient Boosting  
- XGBoost  

**Techniques Applied**  
- Encoding: TF-IDF, BERT embeddings  
- Dimensionality Reduction: Truncated SVD, SelectKBest using chi-square statistics  
- Sampling Methods: SMOTE and ADASYN for oversampling the minority class  
- Hyperparameter Optimization: GridSearchCV  
- Evaluation Metrics: Accuracy, Precision, Recall, F1-Score, ROC-AUC  

## Summary of Results

The experiments show that ensemble models, particularly AdaBoost and XGBoost, achieved high performance with TF-IDF vectors and TruncatedSVD. For BERT-based features, models such as Logistic Regression and Random Forest performed better when paired with ADASYN oversampling, which synthesizes minority class instances more adaptively than SMOTE or random oversampling. The accuracy metric was chosen due to the balanced dataset achieved post-processing, along with F1-score to account for precision-recall balance.

## Literature Survey

Multiple methodologies for hallucination detection have been proposed in recent research. SelfCheckGPT detects hallucinations by evaluating consistency across multiple responses. Knowledge Consistent Alignment (KCA) aligns generated outputs with known facts from sources like Wikipedia. Embedding Distance Analysis measures the distance between generated and reference sentence embeddings. Retrieval-Augmented Generation (RAG) enhances factual grounding by retrieving knowledge during generation. Reinforcement Learning with Human Feedback (RLHF) has also been proposed to fine-tune models based on factual feedback. Other methods include factual probing, prompt-based validation, and fact-checking with LLMs. Metrics commonly used to evaluate hallucination detection include factual consistency score, BLEU, BERTScore, and human evaluations. Studies also highlight that hallucination behavior varies across models (e.g., GPT-3 vs. LLaMA), and that domain shift and low-resource settings pose significant challenges.

## How to Use

1. Clone the repository:
   ```
   git clone https://github.com/your-username/hallucination-detection.git
   cd hallucination-detection
   ```

2. Install the required dependencies:
   ```
   pip install -r requirements.txt
   ```

3. Open the notebook:
   ```
   jupyter notebook hallucination_detection.ipynb
   ```

4. Run all cells to reproduce the results.

## Future Work

Future improvements may include integrating larger benchmark datasets like TruthfulQA, expanding into domain-specific hallucinations, incorporating multimodal hallucination detection, and fine-tuning LLMs with reinforcement learning approaches. Cross-model agreement and ensemble detection from multiple LLM outputs can further enhance robustness.
